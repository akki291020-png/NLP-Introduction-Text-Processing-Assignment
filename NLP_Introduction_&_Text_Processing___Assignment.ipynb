{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Introduction & Text Processing Assignment **\n",
        "\n",
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "sol) At its core, Computational Linguistics is a branch of linguistics that uses computer science to understand how language works. It’s less about building a \"product\" and more about using mathematical models to explore the structure, logic, and evolution of human language.The Goal: To model the \"why\" and \"how\" of language.The Focus: Syntax (sentence structure), semantics (meaning), phonology (sounds), and morphology (word formation).The Approach: Historically, it relied heavily on rule-based systems (like formal grammars), though it now embraces statistical and neural models to test linguistic theories.2. How it Relates to NLPNatural Language Processing (NLP) is the engineering-driven sibling of CL. While CL asks \"How does language work?\", NLP asks \"How can I get a computer to do something useful with this text?\"The relationship is essentially Theory vs. Application:FeatureComputational Linguistics (CL)Natural Language Processing (NLP)Primary GoalScientific discovery and modeling.Solving tasks and building tools.Success MetricDoes the model accurately represent human language?Does the system perform the task efficiently?Typical TasksParsing, discourse analysis, historical linguistics.Sentiment analysis, translation, chatbots.Key DisciplineHumanities-heavy (Linguistics).Engineering-heavy (Computer Science).3. The \"Sweet Spot\" IntersectionIn the modern era, the line between them has blurred significantly. To build a Great Language Model (like the one I'm using to talk to you right now), you need both:CL provides the understanding of nuance, context, and grammar.NLP provides the massive neural networks and processing power to handle billions of words.Basically, Computational Linguistics provides the blueprint, and NLP builds the house.\n",
        "\n",
        "Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "sol) The history of NLP is a fascinating journey from rigid, logic-based rules to the \"black box\" of modern neural networks. It’s essentially the story of moving from teaching a computer the rules of language to letting the computer experience language.\n",
        "\n",
        "1. The Era of Rules (1950s – 1980s)\n",
        "Early NLP was dominated by the belief that language could be solved with pure logic. Scientists tried to hand-code every grammatical rule.\n",
        "+1\n",
        "\n",
        "The Turing Test (1950): Alan Turing proposed his famous test for machine intelligence, setting the stage for conversational AI.\n",
        "\n",
        "The Georgetown Experiment (1954): A famous (and overly optimistic) attempt to automatically translate sixty Russian sentences into English.\n",
        "+1\n",
        "\n",
        "ELIZA (1966): One of the first \"chatterbots,\" which mimicked a therapist by reflecting the user’s words back at them using simple pattern matching.\n",
        "\n",
        "2. The Statistical Revolution (1990s – 2010s)\n",
        "By the 90s, researchers realized that language is too messy for fixed rules. Instead of telling the computer how language should work, they started using probability.\n",
        "\n",
        "Corpus Linguistics: Researchers began using massive \"corpora\" (text datasets) to calculate the likelihood of certain words appearing together.\n",
        "\n",
        "Machine Translation: IBM’s statistical models started outperforming rule-based systems by looking for patterns in translated documents (like UN proceedings).\n",
        "\n",
        "3. The Neural & Deep Learning Wave (2010s – 2017)\n",
        "The rise of \"Deep Learning\" changed everything. Instead of human-engineered statistics, we started using Neural Networks inspired by the human brain.\n",
        "\n",
        "Word Embeddings (Word2Vec): This allowed computers to understand that \"king\" and \"queen\" are related by representing words as mathematical vectors in a high-dimensional space.\n",
        "\n",
        "RNNs and LSTMs: These models were designed to \"remember\" the beginning of a sentence while reading the end, which is crucial for context.\n",
        "\n",
        "4. The Transformer Era (2017 – Present)\n",
        "This is where we are now. The invention of the Transformer architecture (from the paper \"Attention Is All You Need\") catalyzed a massive leap in capability.\n",
        "\n",
        "Attention Mechanisms: Instead of reading word-by-word, models can now \"look\" at an entire paragraph at once to understand context.\n",
        "\n",
        "Large Language Models (LLMs): Models like GPT-4 and Gemini are trained on nearly the entire internet, allowing them to not just process language, but to reason and generate creative content.\n",
        "\n",
        "Fun Fact: In the 1950s, experts predicted machine translation would be \"solved\" within three to five years. It ended up taking closer to seventy!\n",
        "\n",
        "Question 3: List and explain three major use cases of NLP in today’s tech industry.\n",
        "sol) In 2026, NLP has moved beyond simple \"word processing\" to become the cognitive engine behind most modern business operations. Here are three major use cases that have become industry standards:\n",
        "\n",
        "1. Autonomous AI Agents (Conversational AI 2.0)\n",
        "While the chatbots of the early 2020s could only answer questions, 2026-era Autonomous Agents can execute multi-step tasks. These systems use NLP to understand complex instructions and \"reason\" through a workflow.\n",
        "\n",
        "How it works: An agent doesn't just tell you your flight is delayed; it understands the sentiment of your frustration, looks up alternative flights, checks your calendar for conflicts, and asks if you'd like it to rebook the best option.\n",
        "\n",
        "Industry Impact: Customer support is shifting from \"FAQ bots\" to \"Resolution bots\" that actually perform the work, drastically reducing the need for human intervention in routine logistics.\n",
        "\n",
        "2. Intelligent Document Processing (IDP)\n",
        "In heavily regulated industries like Finance, Law, and Healthcare, NLP is used to transform mountains of unstructured text (contracts, medical notes, emails) into structured, actionable data.\n",
        "\n",
        "How it works: Instead of a human reading 500 pages of a legal merger, NLP models use Named-Entity Recognition (NER) and Summarization to automatically flag high-risk clauses, extract expiration dates, and cross-reference them with current regulations.\n",
        "\n",
        "Industry Impact: This has automated up to 75% of manual data entry and compliance auditing, allowing professionals to focus on strategy rather than \"paper-pushing.\"\n",
        "\n",
        "3. Real-Time Multimodal Semantic Search\n",
        "Search has evolved from matching \"keywords\" to understanding \"intent\" across different types of media. In 2026, tech companies use NLP to power search engines that treat text, voice, and video as a single searchable landscape.\n",
        "\n",
        "How it works: A user can search for a specific moment in a 10-hour video archive by describing the concept (e.g., \"The part where the CEO talks about the 2027 sustainability roadmap\"). NLP models index the audio transcripts and visual cues to find the exact timestamp.\n",
        "\n",
        "Industry Impact: This is revolutionizing E-commerce (allowing users to find products via conversational descriptions) and Enterprise Knowledge Management (allowing employees to \"ask\" their company's internal data for answers).\n",
        "\n",
        "Summary Table\n",
        "| Use Case | Core Tech | Primary Benefit |\n",
        "| :--- | :--- | :--- |\n",
        "| Autonomous Agents | LLMs & Planning Algorithms | End-to-end task execution. |\n",
        "| Document Intelligence | NER & Summarization | Speeding up compliance and data entry. |\n",
        "| Semantic Search | Vector Embeddings | Finding information by meaning, not words. |\n",
        "\n",
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "sol) Think of Text Normalization as the \"cleanup crew\" of the NLP world. It is the process of transforming raw, messy text into a consistent, standard format that a computer can actually understand and process without getting confused by superficial variations.1. What exactly is Text Normalization?Human language is full of \"noise\"—we use capital letters, punctuation, slang, and different tenses that all point to the same core idea. Text normalization strips away this noise to reveal the underlying meaning.It typically involves several key steps:Tokenization: Breaking a sentence into individual words or \"tokens.\"Case Folding: Converting everything to lowercase so that \"Apple,\" \"APPLE,\" and \"apple\" are treated as the same word.Stopword Removal: Filtering out common words that don't add much meaning (like \"the,\" \"is,\" and \"at\").Stemming & Lemmatization: Reducing words to their root form. For example, \"running,\" \"ran,\" and \"runs\" all become \"run.\"Noise Removal: Stripping out HTML tags, special characters, or emojis if they aren't relevant to the analysis.2. Why is it Essential?Without normalization, a computer sees the world in a very fragmented way. Here is why we can't skip it:A. Reducing DimensionalityIf a model treats \"Go,\" \"go,\" and \"going\" as three entirely different concepts, your \"vocabulary\" becomes unnecessarily massive. By normalizing them to a single root, you make the data more dense and the model more efficient.B. Improving ConsistencyImagine you are building a search engine. If a user searches for \"running shoes\" but your database only has the phrase \"run shoe,\" a system without normalization might fail to find a match. Normalization ensures the intent matches the data.C. Boosting AccuracyIn tasks like Sentiment Analysis, keeping punctuation or extra spaces can confuse a model. Normalization ensures the machine focuses on the \"signal\" (the words that carry emotion) rather than the \"noise\" (the formatting).3. A Quick ComparisonLook at how a single sentence changes after a standard normalization pipeline:Original Text\"The Quick Brown Foxes are jumping over 2 Lazy Dogs!!\"Lowercasing\"the quick brown foxes are jumping over 2 lazy dogs!!\"Punctuation Removal\"the quick brown foxes are jumping over 2 lazy dogs\"Stopword Removal\"quick brown foxes jumping 2 lazy dogs\"Lemmatization\"quick brown fox jump 2 lazy dog\"Pro Tip: In modern LLMs (like GPT-4), we actually do less aggressive normalization than we used to, because these massive models are smart enough to understand that \"Foxes\" and \"fox\" are related. However, for traditional machine learning and search indexing, it remains a critical foundation.\n",
        "\n",
        "\n",
        "Question 5: Compare and contrast stemming and lemmatization with suitable\n",
        "examples.\n",
        "sol) While both stemming and lemmatization aim to reduce a word to its base form, they go about it in very different ways. One is a \"hacker\" that chops off the ends of words, while the other is a \"linguist\" that understands the context and dictionary definition.1. Stemming: The \"Chop-and-Drop\" ApproachStemming is a heuristic process that lops off the ends of words in the hope of reaching a common root. It follows a set of crude rules (like the famous Porter Stemmer) and doesn't actually \"know\" the language.+1How it works: It uses algorithms to remove common suffixes like \"-ing,\" \"-ed,\" or \"-ies.\"Pros: It is extremely fast and requires very little memory.Cons: It often results in \"non-words\" that don't exist in the dictionary (e.g., \"univers\").Example:The word \"Caring\" becomes \"Car\". (Note: This is an error, as \"caring\" relates to \"care,\" not \"car.\")2. Lemmatization: The \"Dictionary\" ApproachLemmatization is a more sophisticated process that uses a vocabulary and morphological analysis to return the lemma (the dictionary form of a word).How it works: It looks at the Part of Speech (POS) of a word. It knows that \"saw\" could be a tool (noun) or the past tense of \"see\" (verb).Pros: It is highly accurate and always returns a valid word.Cons: It is computationally \"expensive\" (slower) because it has to look up words in a large database like WordNet.Example:The word \"Caring\" becomes \"Care\". (Correct linguistic root.)3. Side-by-Side ComparisonFeatureStemmingLemmatizationLogicRule-based (suffix stripping).Morphological analysis (dictionary lookup).SpeedVery Fast.Slower (requires more processing).OutputCan be non-dictionary fragments.Always a valid, meaningful word.ContextIgnores context.Considers Part of Speech (POS tag).Best Use CaseLarge-scale search indexing.Chatbots, Translation, Sentiment Analysis.4. Visualizing the DifferenceTo truly see the contrast, look at how both methods handle the irregular verb \"Went\":Stemming: Returns \"Went\". (It doesn't see a suffix to chop, so it does nothing.)Lemmatization: Returns \"Go\". (It recognizes that \"went\" is the past tense of \"go.\")Another classic example is \"Better\":Stemming: Returns \"Better\".Lemmatization: Returns \"Good\".The Verdict: If you need speed and are working with massive datasets where perfect grammar doesn't matter (like a basic search engine), use Stemming. If you are building a sophisticated AI that needs to understand the actual meaning of a sentence, use Lemmatization.\n",
        "\n",
        "Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n"
      ],
      "metadata": {
        "id": "AX2Vl3KR_yf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# The block of text\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\n",
        "\"\"\"\n",
        "\n",
        "# Define the regex pattern for a standard email address\n",
        "# This pattern matches:\n",
        "# - Alphanumeric characters, dots, underscores, plus signs, dashes before the @\n",
        "# - Domain name\n",
        "# - Top-level domain (e.g., .com, .org, .co.us)\n",
        "email_pattern = r'[a-zA-Z0-9._+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Use re.findall to extract all matches\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLX_YRUvBRTb",
        "outputId": "71198d2b-79f9-4eaf-ae36-6e7300123ece"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”\n",
        "\n"
      ],
      "metadata": {
        "id": "OSYZaYdEBVND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# The sample paragraph\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Tokenization: Convert text to lowercase and split into words\n",
        "# We use .lower() to ensure 'Natural' and 'natural' are treated the same.\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# 2. Filtering: Remove punctuation and stop words (common words like 'is', 'the')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "filtered_tokens = [\n",
        "    word for word in tokens\n",
        "    if word not in stop_words and word not in punctuation\n",
        "]\n",
        "\n",
        "# 3. Frequency Distribution\n",
        "frequency_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Print the top 5 most common words\n",
        "print(\"Top 5 Most Frequent Words:\")\n",
        "for word, frequency in frequency_dist.most_common(5):\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "# 4. Optional: Print the full distribution\n",
        "# print(\"\\nFull Frequency Distribution:\")\n",
        "# print(frequency_dist.items())"
      ],
      "metadata": {
        "id": "mAOw95eUBj5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text.\n"
      ],
      "metadata": {
        "id": "H-Ab47kCBk-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# 1. Register a custom extension on the Token class\n",
        "# This allows us to store our own 'is_proper_noun' property on every token\n",
        "Token.set_extension(\"is_proper_noun\", default=False, force=True)\n",
        "\n",
        "# 2. Define the Custom Component\n",
        "@Language.component(\"proper_noun_annotator\")\n",
        "def proper_noun_annotator(doc):\n",
        "    # Iterate through every token in the document\n",
        "    for token in doc:\n",
        "        # 'PROPN' is the standard spaCy tag for Proper Nouns\n",
        "        if token.pos_ == \"PROPN\":\n",
        "            token._.is_proper_noun = True\n",
        "    return doc\n",
        "\n",
        "# 3. Load the model and add our component to the pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"proper_noun_annotator\", last=True)\n",
        "\n",
        "# 4. Test the annotator\n",
        "text = \"Google was founded by Larry Page and Sergey Brin in California.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"{'Token':<15} | {'POS':<6} | {'Custom Annotator'}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<15} | {token.pos_:<6} | {token._.is_proper_noun}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IMaLp8IDg14",
        "outputId": "ab43c568-b538-4568-d1cb-f94b4ac66747"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token           | POS    | Custom Annotator\n",
            "----------------------------------------\n",
            "Google          | PROPN  | True\n",
            "was             | AUX    | False\n",
            "founded         | VERB   | False\n",
            "by              | ADP    | False\n",
            "Larry           | PROPN  | True\n",
            "Page            | PROPN  | True\n",
            "and             | CCONJ  | False\n",
            "Sergey          | PROPN  | True\n",
            "Brin            | PROPN  | True\n",
            "in              | ADP    | False\n",
            "California      | PROPN  | True\n",
            ".               | PUNCT  | False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n"
      ],
      "metadata": {
        "id": "V9gUXGiHDiTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# 1. The Dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# 2. Tokenization and Preprocessing\n",
        "# simple_preprocess converts text to lowercase, tokenizes it,\n",
        "# and removes punctuation/numbers.\n",
        "processed_dataset = [simple_preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "# 3. Train the Word2Vec Model\n",
        "# vector_size: Dimensions of the word vectors\n",
        "# window: Context window size (words to the left and right)\n",
        "# min_count: Ignores words with total frequency lower than this\n",
        "# epochs: Number of iterations over the corpus\n",
        "model = Word2Vec(sentences=processed_dataset,\n",
        "                 vector_size=100,\n",
        "                 window=5,\n",
        "                 min_count=1,\n",
        "                 workers=4,\n",
        "                 epochs=10)\n",
        "\n",
        "# 4. Test the model: Find similar words\n",
        "word = \"language\"\n",
        "if word in model.wv:\n",
        "    similar_words = model.wv.most_similar(word, topn=3)\n",
        "    print(f\"Words similar to '{word}': {similar_words}\")\n",
        "else:\n",
        "    print(f\"'{word}' not found in vocabulary.\")"
      ],
      "metadata": {
        "id": "F-qL1trnDrTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "sol) As a data scientist in a fintech startup, handling raw customer feedback requires a pipeline that transforms chaotic language into actionable business metrics. I would approach this task by building an NLP pipeline structured into three core phases: Ingestion & Cleaning, NLP Processing, and Insight Extraction.\n",
        "\n",
        "1. Ingestion & Cleaning (Preprocessing)\n",
        "Before analysis, the data must be standardized to remove \"noise\" that doesn't contribute to sentiment or intent.\n",
        "\n",
        "Handling Raw Data: Combine reviews from various sources (App Store, emails, support tickets) into a single structured format.\n",
        "\n",
        "Normalization: Convert text to lowercase, remove URLs, HTML tags, and special characters.\n",
        "\n",
        "Tokenization & Cleaning: Split text into words and remove stopwords (common words like \"the\", \"and\") that don't carry significant meaning for financial sentiment.\n",
        "\n",
        "Lemmatization: Reduce words to their dictionary roots (e.g., \"banking,\" \"banked,\" and \"banks\" all become \"bank\") to reduce vocabulary size.\n",
        "\n",
        "2. NLP Processing (Modeling)\n",
        "In this phase, we apply algorithms to understand the structure and emotional tone of the feedback.\n",
        "\n",
        "Sentiment Analysis: Use a model (like a fine-tuned BERT model) to classify reviews as Positive, Negative, or Neutral. This helps quantify overall customer satisfaction.\n",
        "\n",
        "Named-Entity Recognition (NER): Identify specific entities mentioned, such as product names (\"Savings Account\", \"Stock Trading\") or competitor names.\n",
        "\n",
        "Topic Modeling (LDA): Use Latent Dirichlet Allocation to automatically cluster reviews into overarching themes (e.g., \"App Crashes,\" \"High Fees,\" \"Customer Service\") without having to read every single review.\n",
        "\n",
        "3. Insight Extraction (Actionable Intelligence)\n",
        "Finally, we translate the model outputs into business strategy.\n",
        "\n",
        "Driver Analysis: Correlate specific topics with negative sentiment. For example: Do reviews mentioning \"transfer speed\" correlate strongly with negative feedback?\n",
        "\n",
        "Dashboarding: Build a real-time dashboard tracking sentiment trends over time and the volume of specific complaints.\n",
        "\n",
        "Prioritization for Product Team: Provide a ranked list of issues based on sentiment severity and frequency to guide the roadmap.\n",
        "\n",
        "Summary Checklist for a Fintech Pipeline\n",
        "Data: Unstructured Text\n",
        "\n",
        "Tools: Python, spaCy, Hugging Face Transformers\n",
        "\n",
        "Goal: Turn unstructured text into structured sentiment metrics"
      ],
      "metadata": {
        "id": "9-mAQ_tWDr43"
      }
    }
  ]
}